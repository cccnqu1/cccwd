<html>
<head>
  <meta charset="utf-8" />
</head>
<body>
<script src="../../gospa.js"></script>
<title> 陳鍾誠  / 教科書  / 機率統計 / 期望值與動差生成函數</title>

 [[陳鍾誠]](ccc:home)  / [[教科書]](book:home)  / [[機率統計]](st:home) 

<h1 id="">期望值與動差生成函數</h1>

<h2 id="">期望值</h2>

<p>定義：期望值 E(X) , (通常用符號 $\mu$ 代表, $\mu=E(X)$)</p>

<blockquote>
  <p>離散分布： $E[X] = \sum_{s \in S, x=X(s)} x P(x)$  　　　　; 通常簡寫為 $\sum_{x \in X(S)} x P(x)$ 或者直接寫 $\sum_{\forall x} x P(x)$　</p>
  
  <p>連續分布： $E[X] = \int_{-\infty}^{\infty} x f(x) dx$</p>
</blockquote>

<p>期望值的相關定理：</p>

<blockquote>
  <ol>
  <li><p>$E[c] = c$ ;</p></li>
  <li><p>$E[c X] = c E[X]$ ;</p></li>
  <li><p>$E[X + Y] = E[X] + E[Y]$ ;</p></li>
  </ol>
</blockquote>

<p>證明</p>

<p>定理 1: E[c] = c</p>

<blockquote>
  <p>$E[c] = \sum_{x \in X(S)} (c P(x))$ ; 根據期望值定義</p>
  
  <p>$= c\;\sum_{x \in X(S)} P(x)$  ; 根據基本算術</p>
  
  <p>$= c$                       ; 因為 P(x) 是機率密度函數</p>
</blockquote>

<p>定理 2: E[c X] = c E[X]</p>

<blockquote>
  <p>$E[c X] = \sum_{x \in X(S)} (c  x  P(x))$  ; 根據期望值定義</p>
  
  <p>$= c\;\sum_{x \in X(S)} (x  P(x))$  ; 根據基本算術</p>
  
  <p>$= c\;E[X]$                    ; 根據期望值定義</p>
</blockquote>

<p>定理 3 : E[X + Y] = E[X] + E[Y]</p>

<blockquote>
  <p>假如離散隨機變數 X, Y 的機率密度函數分別用 $P(X) , P(Y)$ 代表。</p>
  
  <p>$E[X+Y] = \sum_{s \in S, x=X(S), y=Y(s)} (x P(x) + y P(y))$  ; 根據期望值定義</p>
  
  <p>$= \sum_{x \in X(S)} (x P(X)) + \sum_{y \in Y(S)} (y P(y))$ ;  根據乘法對加法的分配率</p>
  
  <p>$=E(X) + E(Y)$ ;</p>
</blockquote>

<p>以上證明了離散的情況，連續的情況雷同，請比照上述寫法撰寫。</p>

<h2 id="">變異數</h2>

<p>定義：變異數 Var(X)</p>

<blockquote>
  <p>離散隨機變數 X 的變異數 Var(X) 定義如下</p>
  
  <p>$Var(X) = \sigma^2 = E[(X-\mu)^2] = \sum_{x \in X(S)} (x-\mu)^2 P(x)$</p>
  
  <p>說明：</p>
  
  <ol>
  <li>上式中的 Var(X) 稱為 X 的變異數，而其平方根 $\Sigma$ 稱為 X 的標準差。 ( $\mu$ 為 X 的期望值)</li>
  <li>以上算式中 $\sum$ 的下標均為 $x \in X(S)$，而非 $x \in S$，也就是 x 是實數值，而非樣本點。</li>
  <li>這也是為何要將隨機變數定義為實函數的原因，這樣才能對這些「變數」進行 +, -, * > 等代數運算，並且可以進行期望值與變異數的計算。</li>
  </ol>
</blockquote>

<p>定理： $Var(X) = E[X^2] - (E[X])^2$ </p>

<blockquote>
  <p>$Var(X) = E[(X-\mu)^2]$ ;</p>
  
  <p>$= E[X^2 - 2 \mu X + \mu^2]$ ;</p>
  
  <p>$= E[X^2] - 2 \mu E[X] + \mu^2$ ;</p>
  
  <p>$= E[X^2] - 2 E[X] E[X] + E[X]^2$ ;</p>
  
  <p>$= E[X^2] - E[X]^2$ .</p>
</blockquote>

<h2 id="">不偏估計式</h2>

<p>定義：(不偏估計式) 若 $E[\hat{\theta}]=\theta$ 則稱估計式 $\hat{\theta}$ 為 $\theta$ 的不偏估計式。</p>

<h2 id="">平均值的估計</h2>

<blockquote>
  <p>定理： $\bar{x}=\frac{x_1+x_2+...+x_n}{n}$ 是 $E(X)=\mu$ 的不偏估計式。</p>
  
  <p>證明：</p>
  
  <p>$E(\bar{x}) = E(\frac{x_1+x_2+...+x_n}{n}) = \frac{E(x_1)+E(x_2)+....+E(x_n)}{n} = \frac{n E(X)}{n} = E(X)$ </p>
</blockquote>

<h2 id="">樣本變異數</h2>

<p>樣本變異數的定義如下：</p>

<p>$S^2=\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}$</p>

<p>定理： $S^2$ 是 $var(x)=\sigma^2=E[(X-\mu)^2]$ 的不偏估計式。</p>

<p>證明：</p>

<blockquote>
  <p>$E[S^2] = E[\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}]$ </p>
  
  <p>$=\frac{E[\sum_{i=1}^n (x_i-\bar{x})^2]}{n-1}$ ; </p>
  
  <p>$= \frac{E[\sum_{i=1}^n ((x_i-\mu)-(\bar{x}-\mu))^2]}{n-1}$ ;</p>
  
  <p>$= \frac{1}{n-1} E[\sum_{i=1}^n ((x_i-\mu)^2-2(x_i-\mu)(\bar{x}-\mu)+(\bar{x}-\mu)^2)]$ ;</p>
  
  <p>$= \frac{1}{n-1} E[\sum_{i=1}^n (x_i-\mu)^2-2(\bar{x}-\mu) \sum_{i=1}^n (x_i-\mu)+n (\bar{x}-\mu)^2)]$ ;</p>
  
  <p>$= \frac{1}{n-1} E[\sum_{i=1}^n (x_i-\mu)^2-2 n (\bar{x}-\mu)^2+n (\bar{x}-\mu)^2)]$ ;</p>
  
  <p>$= \frac{1}{n-1} E[\sum_{i=1}^n (x_i-\mu)^2-n (\bar{x}-\mu)^2]$ ;</p>
  
  <p>$= \frac{1}{n-1} (\sum_{i=1}^n E[(x_i-\mu)^2]-n E[(\bar{x}-\mu)^2])$ ;</p>
  
  <p>$= \frac{1}{n-1} (\sum_{i=1}^n E[(x_i-\mu)^2]-n E[(\bar{x}-\mu)^2])$ ;</p>
  
  <p>$= \frac{1}{n-1} (\sum_{i=1}^n \; \sigma_{x_i}^2-n \sigma_{\bar{x}}^2)$ ; 但是由於 $\sigma_{x_i}^2 = \sigma^2$ 且 $\sigma_{\bar{x}}^2=\frac{\sigma}{n}$</p>
  
  <p>$= \frac{1}{n-1} (n \sigma^2 - \sigma^2) = \frac{1}{n-1} ((n-1)\sigma^2) = \sigma^2$ ; </p>
</blockquote>

<p>所以得證 $S^2$ 是 $\sigma^2$ 的不偏估計式。</p>

<h2 id="">期望值的函數</h2>

<p>有時，我們會想計算某個隨機變數之函數的期望值，像是 E[g(X)]。</p>

<p>某隨機變數 X 之函數 g(X) 的期望值</p>

<blockquote>
  <p><em>期望值 E[g(X)]：</em></p>
  
  <p>$E[g(X)] = \sum_{x \in X(S)} g(x) P(x)$</p>
</blockquote>

<p>舉例而言， $E[X^3 + 2 X^2 + 3 X + 2]$ 就是一個隨機變數 X 的函數 $g(x)=X^3 + 2 X^2 + 3 X + 2$ 的期望值。</p>

<p>而且、由於上述定理 1, 2, 3 的特性，這些期望值的函數還可以拆開來算，舉例如下：</p>

<blockquote>
  <p>$E[X^3 + 2 X^2 + 3 X + 2]$ ;</p>
  
  <p>$=E[X^3] + E[2 X^2] + E[3 X] + E[2]$ ;</p>
  
  <p>$=E[X^3] + 2 E[X^2] + 3 E[X] + 2$ .</p>
</blockquote>

<p>在以上的範例中， $E[X]$ 稱為 X 的 1 級動差， $E[X^2]$ 稱為 X 的 2 級動差， $E[X^3]$ 稱為 X 的 3 級動差 ....</p>

<h2 id="kkthordinarymoment">k 階動差 (Kth Ordinary Moment)</h2>

<blockquote>
  <p>定義： $E[X^k]$ 稱為隨機變數 X 的 k 階動差 (Kth ordinary moment)</p>
</blockquote>

<p>動差的慨念就像是期望值的多項式，我們可以將任何一個多項式的動差寫成 k 個動差的組合，這樣就能將任何的函數的動差給支解。</p>

<p>但是、可惜的是，即使我們將函數分解成動差的組合，其計算上仍然是相當複雜的，但是如果我們只是想變任某個期望值函數對應的原始機率分布為何，那麼可以藉助「動差生成函數」來完成這項任務，以下是動差生成函數的定義。</p>

<h2 id="">動差生成函數</h2>

<blockquote>
  <p>定義：隨機變數 X 的動差生成函數 (Moment Generating Function, m.g.f) $m_X(t)$ 為以下函數</p>
  
  <p>$m_X(t) = E[e^{tX}] = E(1+ t X + \frac{(t X)^2}{2!} + .... + \frac{(t X)^k}{k!}+ .....)$</p>
  
  <p>動差存在的條件是期望值 $E[e^{tX}]$ 在開區間 (-h, h) 內是有限的。</p>
</blockquote>

<p>根據以上定義，離散分布與連續分布的動差生成函數分別可以寫成以下算式：</p>

<blockquote>
  <p>離散分布： $E(e^{t X}) = \sum_{x \in S} {e^{t x}} P(x)$</p>
  
  <p>連續分布： $E(e^{t X}) = \int_{-\infty}^{\infty} {e^{t x}} f(x) dx$</p>
</blockquote>

<p>那麼、動差生成函數到底有甚麼用呢？</p>

<p>關於這個問題，可以讓我們回到泰勒展開式這個微積分的概念來看，就能理解「動差生成函數」背後的原理了。</p>

<p>根據泰勒展開式，我們可以將函數 $e^{tX}$ 展開如下：</p>

<p>$e^{tX} = 1+ t X + \frac{(t X)^2}{2!} + .... + \frac{(t X)^k}{k!}+ .....$</p>

<p>您可以看到在上述展開式當中，不管 k 為何，每一項的 $X^k$ 都存在，並不會消失，而且 $X^k$ 的係數為 $\frac{t^k}{k!}$ ，
因此、只要在某個夠小的開區間 (-h, h) 內這個動差生成函數是有限的，那麼隨機變數 $X$ 與函數 $e^{tX}$ 之間將會有對映關係，而機率密度函數 P(X) 與動差生成函數 $E(e^{t X})$ 也可以被證明有一對一的對映關係。</p>

<p>於是、動差生成函數就成了一個機率分布的「指紋」，意思是如果兩個隨機變數 X, Y 的動差生成函數 $E(e^{t X})=E(e^{t Y})$ ，則這兩個機率分布也必然相同。</p>

<p>思考 1：</p>

<blockquote>
  <p>思考：為何動差生成函數可以做為一個機率分布的「指紋」呢？</p>
  
  <p>說明：如果兩個機率分布 P(X) 與 P(Y) 的動差生成函數相同，那麼將意味著 $E(e^{t X})=E(e^{t X})$ ，根據泰勒展開式可得到</p>
  
  <p>$E(1+ t X + .... + \frac{(t X)^k}{k!}+ .....)=E(1+ t Y + .... + \frac{(t Y)^k}{k!}+ .....)$</p>
  
  <p>因此在每一階的動差上， $E[X^k]$ 都與 $E[Y^k]$ 相同，因此這兩個分布也就應該是一樣的了。</p>
</blockquote>

<p>回顧 1：</p>

<blockquote>
  <p>f(x) 在 0 點的泰勒展開式 (麥克羅林級數) 可以作為一個函數的指紋，意思是如果兩個函數的泰勒展開式相同，則這兩個函數必然相同 (這點是高等微積分課程的核心)。</p>
</blockquote>

<p>回顧 2：</p>

<blockquote>
  <p>函數 f(x) 的特徵函數 (Characteristic function) 為 $E(e^{i t X}) = e^i E(e^{t X}) = e^i  E(1+ t X + \frac{(t X)^2}{2!} + .... + \frac{(t X)^k}{k!}+ .....)$</p>
</blockquote>

<h2 id="">結語</h2>

<p>為何數學家要將隨機變數定義成一種函數，然後相樣本映射到實數空間上，而不是直接對樣本進行機率運算呢？筆者認為應該是為了期望值而布的局，因為將樣本映射到實數之後，才能用下列算式計算期望值。</p>

<blockquote>
  <p>$E[X] = \sum_{x \in X(S)} x P(x)$</p>
</blockquote>

<p>而隨機變數之間的代數運算，像是 「3X」 , 「X+Y」 , 「X-2Y」 ,「<code>X*Y</code>」, 「<code>X*X*X*X</code>」等 運算的結果，也仍然是一種作用在樣本空間 S 的實函數，只是當 X, Y 兩者的樣本空間有所不同時，我們必須以兩者樣本空間的迪卡兒乘積 $S = (S_X, S_Y)$ 作為樣本空間。</p>

<p>在這種情況下，期望值函數也才能運作在 +, - * 等運算空間中，得到以下的廣義期望值：</p>

<blockquote>
  <p>$E[g(X)] = \sum_{x \in X(S)} g(x) P(x)$</p>
</blockquote>

<p>「動差生成函數」可以做為機率分布的指紋，因此如果兩個機率分布的「動差生成函數」相同，那麼其機率分布也會相同。</p>

<p>「動差生成函數」的定義如下：</p>

<blockquote>
  <p>$m_X(t) = E[e^{tX}] = E(1+ t X + \frac{(t X)^2}{2!} + .... + \frac{(t X)^k}{k!}+ .....)$</p>
</blockquote>

</body>
</html>