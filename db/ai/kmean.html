<html>
<head>
  <meta charset="utf-8" />
</head>
<body>
<script src="../../gospa.js"></script>
<title> 陳鍾誠  / 教科書  / 人工智慧 / K-Mean 分群演算法</title>

 [[陳鍾誠]](ccc:home)  / [[教科書]](book:home)  / [[人工智慧]](ai:home) 

<h1 id="kmean">K-Mean 分群演算法</h1>

<ul>
<li><a href="http://ccckmit.wikidot.com/ai:kmeans">http://ccckmit.wikidot.com/ai:kmeans</a></li>
</ul>

<h2 id="">簡介</h2>

<p>K-Means 是 J. B. MacQueen 於1967年所提出的分群演算法，必須事前設定群集的數量 k，然後找尋下列公式的極大值，以達到分群的最佳化之目的。</p>

<p>$argmin \sum_{i=1}^{k} \sum_{\mathbf x_j \in S_i} \big\| \mathbf x_j - \boldsymbol\mu_i \big\|^2$</p>

<p>其中的 $\mu_i$ 是 Si 群體的平均。</p>

<h2 id="">演算法</h2>

<ul>
<li><ol><li>隨機指派群集中心：(圖一)</li>
<li>在訓練組資料中「隨機」找出K筆紀錄來作為初始種子(初始群集的中心)</li></ol></li>
<li><ol><li>產生初始群集：(圖二)</li>
<li>計算每一筆紀錄到各個隨機種子之間的距離，然後比較該筆紀錄究竟離哪一個隨機種子最近，然後這筆紀錄就會被指派到最接近的那個群集中心，此時就會形成一個群集邊界，產生了初始群集的成員集合</li></ol></li>
<li><ol><li>產生新的質量中心：(圖三)</li>
<li>根據邊界內的每一個案例重新計算出該群集的質量中心，利用新的質量中心取代之前的隨機種子，來做為該群的中心</li></ol></li>
</ul>

<p>$S_i^{(t)} = \left\{ \mathbf x_j : \big\| \mathbf x_j - \mathbf m^{(t)}_i \big\| \leq \big\| \mathbf x_j - \mathbf m^{(t)}_{i^\cdot} \big\| \text{ for all }i^\cdot=1,\ldots,k \right\}$</p>

<ul>
<li><ol><li>變動群集邊界：(圖四)</li>
<li>指定完新的質量中心之後，再一次比較每一筆紀錄與新的群集中心之間的距離，然後根據距離，再度重新分配每一個案例所屬的群集</li></ol></li>
</ul>

<p>$\mathbf m^{(t+1)}_i = \frac{1}{|S^{(t)}_i|} \sum_{\mathbf x_j \in S^{(t)}_i} \mathbf x_j$</p>

<ul>
<li><ol><li>持續反覆 3, 4 的步驟，一直執行到群集成員不再變動為止</li></ol></li>
</ul>

<p><a href="#ai:=image K-MeanEx1.jpg" class="innerLink">=image K-MeanEx1.jpg</a>
= 圖一、隨機指派群集中心</p>

<p><a href="#ai:=image K-MeanEx2.jpg" class="innerLink">=image K-MeanEx2.jpg</a>
= 圖二、產生初始群集</p>

<p><a href="#ai:=image K-MeanEx3.jpg" class="innerLink">=image K-MeanEx3.jpg</a>
= 圖三、產生新的質量中心</p>

<p><a href="#ai:=image K-MeanEx4.jpg" class="innerLink">=image K-MeanEx4.jpg</a>
= 圖四、變動群集邊界</p>

<h2 id="">參考文獻</h2>

<ul>
<li>J. B. MacQueen (1967): "Some Methods for classification and Analysis of Multivariate Observations", Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press, 1:281-297</li>
<li>J. A. Hartigan (1975) "Clustering Algorithms". Wiley.</li>
<li>J. A. Hartigan and M. A. Wong (1979) "A K-Means Clustering Algorithm", Applied Statistics, Vol. 28, No. 1, p100-108.</li>
<li>[http://www.stanford.edu/~darthur D. Arthur]，[http://www.stanford.edu/~sergeiv S. Vassilvitskii] (2006): "How Slow is the k-means Method?," Proceedings of the 2006 Symposium on Computational Geometry (SoCG).</li>
<li>http://en.wikipedia.org/wiki/K-means_clustering</li>
<li>http://zh.wikipedia.org/wiki/K平均演算法</li>
<li>http://140.118.9.173/salo/data%20mining/Ch5 群集演算法.pdf</li>
<li>漫谈 Clustering (1): k-means -- http://blog.pluskid.org/?p=17</li>
</ul>

</body>
</html>